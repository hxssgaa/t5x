# Copyright 2022 The T5X Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __gin__ import dynamic_registration

from t5x import partitioning
from t5x import trainer

from t5x.examples.t5 import network
from t5x import adafactor

include "t5x/contrib/gpu/t5/t5_1_1/large.gin"
# include "dir1/user_dir/t5_1_1_xl.gin"
# include "dir1/user_dir/flash_attention.gin"
# include "t5x/contrib/gpu/t5/t5_1_1/adamw_opt.gin"
include "t5x/contrib/gpu/t5/configs/runs/pretrain_pile.gin"

# Register necessary SeqIO Tasks/Mixtures.
import t5.data.mixtures
# Register Dummy Wikipedia Seqio Task (needed for benchmarking)
import dummy_wikipedia_seqio

# Optimizer
# `learning_rate` is set by `Trainer.learning_rate_fn`.
OPTIMIZER = @adafactor.Adafactor()
adafactor.Adafactor:
  decay_rate = 0.8
  step_offset = 0
adafactor.Adafactor.logical_factor_rules = @adafactor.standard_logical_factor_rules()

MIXTURE_OR_TASK_NAME = "wikipedia_dummy"
TASK_FEATURE_LENGTHS = {"inputs": 512, "targets": 114}
TRAIN_STEPS = 80000
DROPOUT_RATE = 0.0
LABEL_SMOOTHING = 0.0
LOSS_NORMALIZING_FACTOR = None
Z_LOSS = 0.0001
BATCH_SIZE = 2048
USE_CACHED_TASKS=False

trainer.Trainer:
    num_microbatches = 0

#partitioning.standard_logical_axis_rules:
#    activation_partitioning_dims = 2
#    parameter_partitioning_dims = 2

partitioning.PjitPartitioner:
    num_partitions = 2
    # model_parallel_submesh = (1, 2, 4, 1)