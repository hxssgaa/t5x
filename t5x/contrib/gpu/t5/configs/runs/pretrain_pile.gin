include 't5x/contrib/gpu/t5/configs/runs/pretrain.gin'

USE_CACHED_TASKS = False

utils.SaveCheckpointConfig:
  period = 2000
  dtype = 'bfloat16'
  keep = 2  # keep 2 checkpoints
  save_dataset = False  # checkpoint dataset state

# This scheduler is made with adam in mind. Use the scheduler from pretrain.gin if using adafactor
#utils.create_learning_rate_scheduler:
#  factors = 'linear_decay'
#  base_learning_rate = 0.0001
#  warmup_steps = 10000  # 10k to keep consistent with T5/MTF defaults.
#  min_learning_rate = 0.00001
#  decay_factor = 9.0909e-7
utils.create_learning_rate_scheduler:
  factors = 'constant * rsqrt_decay'
  base_learning_rate = 1.0
  warmup_steps = 10000  # 10k to keep consistent with T5/MTF defaults.