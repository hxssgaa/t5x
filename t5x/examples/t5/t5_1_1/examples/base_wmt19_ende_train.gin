from __gin__ import dynamic_registration

import __main__ as train_script
from t5x import adafactor
from t5x import models
from t5x import partitioning
from t5x import trainer
from t5x import utils
from t5x.examples.t5 import network
from t5.data import mixtures

include "t5x/examples/t5/t5_1_1/large.gin"
include "t5x/configs/runs/finetune.gin"

MIXTURE_OR_TASK_NAME = "squad_v010_allanswers"
MIXTURE_OR_TASK_MODULE = "t5.data.mixtures"
TASK_FEATURE_LENGTHS = {"inputs": 956, "targets": 256}
TRAIN_STEPS = 1_006_001
LOSS_NORMALIZING_FACTOR = 262144
INITIAL_CHECKPOINT_PATH = None
# Note that `DROPOUT_RATE = 0.1` is specified in the finetune.gin but we just
# repeat to make it explicit.
DROPOUT_RATE = 0.1

train/utils.DatasetConfig:
  batch_size = 128
  split = 'train'
  use_cached = False
  pack = True
  use_custom_packing_ops = False
  seed = 0
  module = %MIXTURE_OR_TASK_MODULE

train_eval/utils.DatasetConfig:
  batch_size = 128
  split = 'validation'
  shuffle = False
  seed = 42
  use_cached = False
  pack = False
  use_custom_packing_ops = False
  module = %MIXTURE_OR_TASK_MODULE

infer_eval/utils.DatasetConfig:
  task_feature_lengths = None  # compute max
  split = 'validation'
  batch_size = 128
  shuffle = False
  seed = 42
  use_cached = False
  pack = False
  module = %MIXTURE_OR_TASK_MODULE

train_script.train:
  eval_period = 250
  stats_period = 250
  eval_steps = 20
  random_seed = 0
  use_hardware_rng = True

utils.SaveCheckpointConfig:
  period = 3000  # checkpoint frequency
  keep = 1

trainer.Trainer.num_microbatches = None
utils.create_learning_rate_scheduler.warmup_steps = 1001000

partitioning.PjitPartitioner:
  num_partitions = 1

adafactor.Adafactor:
  logical_factor_rules = @adafactor.standard_logical_factor_rules()
